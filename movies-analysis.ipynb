{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b479fce9-d280-42b6-9ea3-b21182e3bd9c",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "This project is my first step into the world of data analysis and understanding what it takes to work with data.\n",
    "\n",
    "The idea behind this project is simple, pick a dataset, learn to clean, analyze and extract statistical insights, then create visualizations that communicate some of the findings.\n",
    "\n",
    "LINK TO DATASET: https://www.kaggle.com/datasets/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows\n",
    "\n",
    "For this mini project, I decided to use a dataset containing the top 1000 IMDB movies from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b5af4",
   "metadata": {},
   "source": [
    "### **Part 1: Data Exploration and Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7b2ed3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7ac65",
   "metadata": {},
   "source": [
    "#### **Data Overview**\n",
    "- how many rows and columns does the dataset have?\n",
    "- what are the data types of each column? are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fedd7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from .csv file\n",
    "data = pd.read_csv(\"imdb_top_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a486ed8",
   "metadata": {},
   "source": [
    "There are 1000 rows and 16 columns, in the dataset. This is prior to cleaning the dataset of unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5655cd9e",
   "metadata": {},
   "source": [
    "Majority of the data types in the dataset are objects (which can contain both characters and numbers). Columns such as `Released_Year`, `Runtime`, `Gross` should be converted to `int64` to make it easier to work with. Additionally, columns such as `Poster_Link`, `Certificate`, `Overview` and `Meta_score` are unnecessary for my use case and could be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680b2fa",
   "metadata": {},
   "source": [
    "The table above shows the number of null values for each column. I will have to find a way to handle null values for `Gross`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06efeb9",
   "metadata": {},
   "source": [
    "There are no duplicate rows in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714b0c9",
   "metadata": {},
   "source": [
    "#### **Data Cleaning**\n",
    "- identify and handle any missing or inconsistent data\n",
    "- are there any outliers in the dataset? if so, how would you handle them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0102f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unnecessary columns\n",
    "data = data.drop(columns=[\"Poster_Link\", \"Certificate\", \"Overview\", \"Meta_score\", \"No_of_Votes\"])\n",
    "\n",
    "# dropping rows where Gross is NaN because no other method makes sense\n",
    "data = data.dropna(subset=\"Gross\")\n",
    "\n",
    "# renaming columns \n",
    "data = data.rename(columns={\"Series_Title\" : \"Movie_Title\", \"Released_Year\" : \"Release_Year\", \"Runtime\" : \"Runtime (in min)\"})\n",
    "\n",
    "# using the Movie_Title as the index instead of 0,1,2...\n",
    "data = data.set_index(\"Movie_Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee9449",
   "metadata": {},
   "source": [
    "After removing any columns that were not needed, renaming some to eliminate confusion and changing the index, did a recheck for any null values that may have been missed and its looking good. Doing a premature check (`data.max()`) to see if the data types should be changed, it is obvious that a Runtime of 99 min and a Gross revenue of $985, 912 make no sense at all and can be confirmed by just reading the first few rows of `data.head(5)`. This confirms that some of the data types should be converted to make sure they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e48c17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any non-numeric characters (e.g., strip whitespaces) and convert to int\n",
    "data['Release_Year'] = pd.to_numeric(data['Release_Year'], errors='coerce').astype('Int64')\n",
    "\n",
    "# extract numeric values from Runtime and convert to int\n",
    "data['Runtime (in min)'] = data['Runtime (in min)'].str.extract(r'(\\d+)').astype('Int64')\n",
    "\n",
    "# remove non-numeric characters (like $, commas) and convert to int\n",
    "data['Gross'] = data['Gross'].replace(r'[\\$,]', '', regex=True).astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df746ba",
   "metadata": {},
   "source": [
    "After performing the conversions and doing a recheck (`data.max()`), the outputs make much more sense. And can further be verified by doing `data.sort_values(by=[\"Gross\"], ascending=False)`. Replace gross with another column name and match with the output of `data.max()` to make sure the values are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=[\"Gross\"], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
